# Llama.cpp サーバー用環境変数サンプル
# モデルファイル名（models/ ディレクトリ内のGGUFファイル名を指定）
LLAMA_MODEL_FILE=gemma3n-e2b-fixed.gguf

# コンテキストサイズ（デフォルト: 2048）
LLAMA_CTX_SIZE=2048

# 並列処理数（デフォルト: 1）
LLAMA_N_PARALLEL=1

# APIポート番号（デフォルト: 8080）
LLAMA_PORT=8080

# GPU版のみ: GPUレイヤー数（例: 35）
# LLAMA_N_GPU_LAYERS=35